{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDybFGVAbk2GaobMHJRVjb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ignas12345/Magistro_projektas/blob/main/bandymas_klasifikuoti.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "cU_eBl0bbHUu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uzkraunam neapdorotus duomenis\n",
        "url = 'https://raw.githubusercontent.com/Ignas12345/Magistro_projektas/refs/heads/main/counts_combined.csv'\n",
        "df = pd.read_csv(url)\n",
        "gene_names = df[\"miRNA_ID\"].to_numpy()\n",
        "df_trimmed = df.drop(df.columns[0], axis=1)\n",
        "full_data = (df_trimmed.to_numpy(dtype=np.int32)).T"
      ],
      "metadata": {
        "id": "dpCO4P51uln6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#arba uzkraunam isfiltruotus duomenis is praeitos knygutes\n",
        "url = 'https://raw.githubusercontent.com/Ignas12345/Magistro_projektas/refs/heads/main/filtruoti_duomenys/filtruoti_1_su_310_miRNR/'\n",
        "filtered_data = (pd.read_csv(url + 'further_filtered_data.csv').values).T\n",
        "gene_names = pd.read_csv(url + 'further_filtered_gene_names.csv', header = None).values.flatten()\n",
        "print(np.shape(filtered_data))\n",
        "print(np.shape(gene_names))"
      ],
      "metadata": {
        "id": "wBcDTeXCmwC1",
        "outputId": "0cc4ecf7-0163-491b-c5e1-9e875ad7bbee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(139, 310)\n",
            "(310,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = full_data"
      ],
      "metadata": {
        "id": "-LKuHl9_h4_m"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create labels: first 6 samples -> label 0, rest -> label 1\n",
        "num_samples = X.shape[0]\n",
        "labels = torch.zeros(num_samples, dtype=torch.float32)\n",
        "labels[6:] = 1  # Set the rest of the samples to label 1\n",
        "\n",
        "# Convert X to a PyTorch tensor\n",
        "features = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "#netikri papildomi treniravimo duomenys\n",
        "benign_features = features[:6]  # First 6 samples are benign\n",
        "benign_labels = labels[:6]\n",
        "\n",
        "# Duplicate benign samples with slight noise\n",
        "augmented_features = benign_features + 0.01 * torch.randn_like(benign_features)\n",
        "features = torch.cat([features, augmented_features])\n",
        "labels = torch.cat([labels, benign_labels])\n",
        "\n",
        "mean = features.mean(dim=0)\n",
        "std = features.std(dim=0)\n",
        "features = (features - mean) / (std + 1e-8)\n",
        "\n",
        "# Combine features and labels into a dataset\n",
        "dataset = TensorDataset(features, labels)"
      ],
      "metadata": {
        "id": "4e0tFFYAcnle"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader for batching\n",
        "batch_size = 1  # You can adjust this as needed\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Example: Print the shape of a batch\n",
        "for batch_features, batch_labels in data_loader:\n",
        "    print(f\"Batch features shape: {batch_features.shape}\")\n",
        "    print(f\"Batch labels shape: {batch_labels.shape}\")\n",
        "    break"
      ],
      "metadata": {
        "id": "_hAWolYbu26q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a46f05-801b-4578-bc54-75e34833b904"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch features shape: torch.Size([1, 1881])\n",
            "Batch labels shape: torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_features, batch_labels in data_loader:\n",
        "    print(f\"Batch features shape: {batch_features}\")\n",
        "    print(f\"Batch labels shape: {batch_labels}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7MFlevNmseU",
        "outputId": "04a174e1-3f54-4243-ed80-9ef24868b4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch features shape: tensor([[ 47204.,  46983.,  47453.,  ...,    408.,   2981., 137966.]])\n",
            "Batch labels shape: tensor([1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseNN(nn.Module):\n",
        "    def __init__(self, input_dim, temperature = 1):\n",
        "        super(SparseNN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.temperature = temperature\n",
        "\n",
        "        self.first_layer_weights = nn.Parameter(torch.randn(input_dim))  # 1D tensor for scaling\n",
        "        self.first_layer_biases = nn.Parameter(torch.randn(input_dim))  # 1D tensor for bias\n",
        "\n",
        "\n",
        "        # Second layer: Sparse weighting (learnable parameters)\n",
        "        self.output_weights_raw = nn.Parameter(torch.randn(input_dim))  # Raw weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First layer: Linear transformation\n",
        "        first_layer_output = x * self.first_layer_weights + self.first_layer_biases\n",
        "\n",
        "        # Second layer: Apply sparse weights\n",
        "        output_weights = torch.relu(self.output_weights_raw)  # Enforce non-negativity\n",
        "        weighted_output = first_layer_output * output_weights  # Element-wise multiplication\n",
        "  # Element-wise multiplication\n",
        "\n",
        "        # Pooling: Sum the weighted outputs\n",
        "        pooled_output = weighted_output.sum(dim=1)  # Summing across features\n",
        "\n",
        "        # Final activation: sigmoid\n",
        "        output = torch.sigmoid(pooled_output / self.temperature)\n",
        "\n",
        "        return output\n",
        "\n",
        "def l1_regularization(weight, lambda_l1=0.001):\n",
        "    return lambda_l1 * weight.abs().sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "eM5yeyfSWIqo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the model\n",
        "input_dim = full_data.shape[1]\n",
        "model = SparseNN(input_dim, temperature = 5)\n",
        "\n",
        "# Define a loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # L1 sparsity via weight_decay"
      ],
      "metadata": {
        "id": "dWKjCuxSlA19"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u0gHb8mXTh7C",
        "outputId": "9f42aa3e-2a61-49c4-b088-2db887e198b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 3.0513\n",
            "Epoch [2/20], Loss: 3.8932\n",
            "Epoch [3/20], Loss: 3.7277\n",
            "Epoch [4/20], Loss: 3.5556\n",
            "Epoch [5/20], Loss: 2.2663\n",
            "Epoch [6/20], Loss: 2.1118\n",
            "Epoch [7/20], Loss: 1.3898\n",
            "Epoch [8/20], Loss: 1.2337\n",
            "Epoch [9/20], Loss: 1.1286\n",
            "Epoch [10/20], Loss: 1.0202\n",
            "Epoch [11/20], Loss: 0.9719\n",
            "Epoch [12/20], Loss: 0.9290\n",
            "Epoch [13/20], Loss: 0.9069\n",
            "Epoch [14/20], Loss: 0.8715\n",
            "Epoch [15/20], Loss: 0.8378\n",
            "Epoch [16/20], Loss: 0.8243\n",
            "Epoch [17/20], Loss: 0.8065\n",
            "Epoch [18/20], Loss: 0.7872\n",
            "Epoch [19/20], Loss: 0.7841\n",
            "Epoch [20/20], Loss: 0.7707\n",
            "Final Accuracy: 0.9931\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "num_epochs = 20  # Adjust based on dataset size and convergence\n",
        "lambda_l1 = 0.001  # Regularization strength\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_features, batch_labels in data_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(batch_features)  # Ensure shape compatibility for BCELoss\n",
        "        # Compute binary cross-entropy loss\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        # Add L1 regularization for sparsity\n",
        "        loss += l1_regularization(model.output_weights_raw, lambda_l1)\n",
        "\n",
        "        #make probabilities less extreme\n",
        "        penalty = (outputs * torch.log(outputs + 1e-8) + (1 - outputs) * torch.log(1 - outputs + 1e-8)).mean()\n",
        "        loss += 0.1 * penalty\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Logging epoch statistics\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(data_loader):.4f}\")\n",
        "\n",
        "# Evaluation (after training)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Forward pass on the entire dataset\n",
        "    predictions = model(features)\n",
        "    predictions = (predictions > 0.5).float()  # Convert probabilities to binary predictions\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(labels.numpy(), predictions.numpy())\n",
        "    print(f\"Final Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "probs = 100 * model(features).detach().numpy()\n",
        "formatted_probs = np.array([f\"{p:.2f}%\" for p in probs])\n",
        "print(formatted_probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxXeME3tiw6N",
        "outputId": "53752d72-5d76-4c49-a4e0-e7ff8b2d0516"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0.28%' '2.63%' '98.76%' '13.91%' '6.85%' '2.21%' '100.00%' '99.84%'\n",
            " '99.97%' '99.61%' '99.25%' '99.98%' '100.00%' '100.00%' '99.84%' '96.65%'\n",
            " '99.59%' '99.98%' '100.00%' '99.96%' '99.95%' '100.00%' '99.56%' '99.91%'\n",
            " '99.99%' '100.00%' '99.87%' '98.75%' '99.90%' '91.62%' '99.97%' '99.85%'\n",
            " '99.57%' '100.00%' '93.07%' '99.83%' '99.97%' '100.00%' '100.00%'\n",
            " '100.00%' '78.69%' '85.09%' '99.97%' '99.86%' '99.94%' '99.91%' '98.92%'\n",
            " '96.46%' '100.00%' '99.58%' '100.00%' '98.58%' '99.46%' '99.59%'\n",
            " '100.00%' '98.60%' '97.74%' '100.00%' '100.00%' '99.84%' '100.00%'\n",
            " '100.00%' '93.83%' '84.41%' '99.85%' '99.96%' '100.00%' '99.79%' '99.79%'\n",
            " '99.93%' '99.92%' '92.36%' '100.00%' '99.76%' '99.98%' '99.78%' '99.87%'\n",
            " '99.77%' '100.00%' '97.00%' '91.85%' '79.52%' '99.78%' '84.40%' '99.07%'\n",
            " '100.00%' '99.98%' '99.56%' '99.98%' '100.00%' '100.00%' '95.92%'\n",
            " '99.38%' '92.33%' '99.79%' '100.00%' '99.97%' '99.04%' '100.00%'\n",
            " '100.00%' '99.99%' '99.54%' '99.99%' '100.00%' '99.99%' '98.96%' '99.99%'\n",
            " '100.00%' '99.87%' '52.97%' '99.90%' '99.96%' '100.00%' '88.59%'\n",
            " '100.00%' '100.00%' '94.31%' '100.00%' '99.94%' '99.78%' '100.00%'\n",
            " '99.98%' '99.98%' '99.12%' '99.00%' '99.99%' '99.99%' '99.99%' '98.38%'\n",
            " '69.99%' '85.16%' '99.99%' '98.10%' '100.00%' '99.99%' '96.14%' '100.00%'\n",
            " '97.74%' '99.48%' '0.00%' '0.12%' '0.00%' '0.05%' '0.00%' '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#jeigu procentai sutampa su tikrove, issaugom svorius (ir jeigu jie naudingi...)\n",
        "\n",
        "torch.save(model.first_layer_weights.data, \"first_layer_weights.pt\")\n",
        "\n",
        "# Save the first layer's biases (if present)\n",
        "torch.save(model.first_layer_biases.data, \"first_layer_biases.pt\")\n",
        "\n",
        "# Save the second layer (sparse) weights\n",
        "torch.save(model.output_weights_raw.data, \"output_weights_raw.pt\")"
      ],
      "metadata": {
        "id": "5cwRWuziTODI"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Svoriu analize is idejos turetu parodyti, kurie genai yra svarbiausi priimant sprendima\n",
        "print(\"First Layer Weights:\")\n",
        "print(model.first_layer_weights.data)\n",
        "\n",
        "# Print the biases of the first layer (if present)\n",
        "print(\"First Layer Biases:\")\n",
        "print(model.first_layer_biases.data)\n",
        "\n",
        "# Print the weights of the second layer (output weights)\n",
        "print(\"Second Layer (Sparse) Weights:\")\n",
        "sparse_weights = torch.relu(model.output_weights_raw.data)\n",
        "print(sparse_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTqC-PIcXGej",
        "outputId": "5cfd252a-dfb0-4426-b1fb-2c71c73ac9a9"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Layer Weights:\n",
            "tensor([-0.0822, -1.0442,  0.2279,  ...,  0.5639,  1.3265,  0.9782])\n",
            "First Layer Biases:\n",
            "tensor([-1.3368,  0.3855,  0.4678,  ...,  0.3809, -1.2594, -1.2489])\n",
            "Second Layer (Sparse) Weights:\n",
            "tensor([0.0000, 0.6077, 2.0900,  ..., 2.4219, 0.0115, 0.5861])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_indices = torch.argsort(sparse_weights, descending=True)\n",
        "values = sparse_weights[sorted_indices]\n",
        "\n",
        "print(sorted_indices[:50])\n",
        "print(values[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he-q2mrITtax",
        "outputId": "a5be63df-63fd-4ae0-950f-a2df464101d8"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1547, 1051, 1484, 1314,  995,  806,  311,  525, 1710, 1588, 1394,  589,\n",
            "        1878,  890, 1769,  887, 1033, 1625,  235, 1832,  365, 1065, 1287,   10,\n",
            "         354,  420,  331,  345,    2,  617,  746, 1411, 1786,  156, 1401, 1831,\n",
            "         482,  602, 1447, 1108,  509,  581, 1791, 1833, 1192,  691,  315,  249,\n",
            "        1640,  497])\n",
            "tensor([3.6505, 3.3982, 3.0856, 2.9888, 2.9321, 2.7831, 2.7825, 2.7048, 2.6162,\n",
            "        2.4972, 2.4582, 2.4468, 2.4219, 2.4009, 2.3941, 2.3351, 2.3335, 2.3317,\n",
            "        2.3224, 2.2581, 2.2448, 2.2206, 2.2016, 2.1306, 2.1150, 2.1146, 2.0958,\n",
            "        2.0937, 2.0900, 2.0758, 2.0651, 2.0588, 2.0584, 2.0507, 1.9739, 1.9727,\n",
            "        1.9717, 1.9564, 1.9521, 1.9478, 1.9326, 1.9235, 1.9224, 1.9220, 1.9107,\n",
            "        1.8866, 1.8859, 1.8831, 1.8786, 1.8785])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SparseNN(input_dim, temperature = 5)\n",
        "\n",
        "loaded_first_layer_weights = torch.load(\"first_layer_weights.pt\")\n",
        "loaded_first_layer_biases = torch.load(\"first_layer_biases.pt\")\n",
        "loaded_otput_weights_raw = torch.load(\"output_weights_raw.pt\")\n",
        "\n",
        "# Assign the loaded values to the model\n",
        "model.first_layer_weights.data = loaded_first_layer_weights\n",
        "model.first_layer_biases.data = loaded_first_layer_biases\n",
        "model.output_weights_raw.data = loaded_otput_weights_raw\n",
        "\n",
        "probs = 100 * model(features).detach().numpy()\n",
        "formatted_probs = np.array([f\"{p:.2f}%\" for p in probs])\n",
        "print(formatted_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSgbW5z_abWH",
        "outputId": "f70c6e5d-0405-4ee5-9a09-969fc846a0aa"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0.28%' '2.63%' '98.76%' '13.91%' '6.85%' '2.21%' '100.00%' '99.84%'\n",
            " '99.97%' '99.61%' '99.25%' '99.98%' '100.00%' '100.00%' '99.84%' '96.65%'\n",
            " '99.59%' '99.98%' '100.00%' '99.96%' '99.95%' '100.00%' '99.56%' '99.91%'\n",
            " '99.99%' '100.00%' '99.87%' '98.75%' '99.90%' '91.62%' '99.97%' '99.85%'\n",
            " '99.57%' '100.00%' '93.07%' '99.83%' '99.97%' '100.00%' '100.00%'\n",
            " '100.00%' '78.69%' '85.09%' '99.97%' '99.86%' '99.94%' '99.91%' '98.92%'\n",
            " '96.46%' '100.00%' '99.58%' '100.00%' '98.58%' '99.46%' '99.59%'\n",
            " '100.00%' '98.60%' '97.74%' '100.00%' '100.00%' '99.84%' '100.00%'\n",
            " '100.00%' '93.83%' '84.41%' '99.85%' '99.96%' '100.00%' '99.79%' '99.79%'\n",
            " '99.93%' '99.92%' '92.36%' '100.00%' '99.76%' '99.98%' '99.78%' '99.87%'\n",
            " '99.77%' '100.00%' '97.00%' '91.85%' '79.52%' '99.78%' '84.40%' '99.07%'\n",
            " '100.00%' '99.98%' '99.56%' '99.98%' '100.00%' '100.00%' '95.92%'\n",
            " '99.38%' '92.33%' '99.79%' '100.00%' '99.97%' '99.04%' '100.00%'\n",
            " '100.00%' '99.99%' '99.54%' '99.99%' '100.00%' '99.99%' '98.96%' '99.99%'\n",
            " '100.00%' '99.87%' '52.97%' '99.90%' '99.96%' '100.00%' '88.59%'\n",
            " '100.00%' '100.00%' '94.31%' '100.00%' '99.94%' '99.78%' '100.00%'\n",
            " '99.98%' '99.98%' '99.12%' '99.00%' '99.99%' '99.99%' '99.99%' '98.38%'\n",
            " '69.99%' '85.16%' '99.99%' '98.10%' '100.00%' '99.99%' '96.14%' '100.00%'\n",
            " '97.74%' '99.48%' '0.00%' '0.12%' '0.00%' '0.05%' '0.00%' '0.00%']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-84-6c24784f2594>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_first_layer_weights = torch.load(\"first_layer_weights.pt\")\n",
            "<ipython-input-84-6c24784f2594>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_first_layer_biases = torch.load(\"first_layer_biases.pt\")\n",
            "<ipython-input-84-6c24784f2594>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_otput_weights_raw = torch.load(\"output_weights_raw.pt\")\n"
          ]
        }
      ]
    }
  ]
}
